\section{Related Work}
There are a number of works that focus on the broad notion of meta learning and memory networks.
However the most important Meta-Learning papers are those that use the Model-Agnostic Meta-Learning (MAML)
framework that underlies MIL. MAML was introduced by Finn et al. in "Model-Agnostic Meta-Learning for
Fast Adaptation of Deep Networks". Subsequently, it was modified
for application to imitation learning in "One-Shot Visual Imitation Learning via Meta-Learning". 
Most recently Finn et al. published "One-Shot Imitation from Observing Humans via Domain-Adaptive Meta-Learning", which extends
a method for imitation learning using only video footage 
i.e. without knowledge of the experts actions.

Similarly, there are a large number of papers that apply memory networks. Therefore, we focus on
works that introduce new types of memory modules. The idea of a memory module was first introduced in
the paper "Neural Turing Machines". Later this concept was improved in "Hybrid computing using a neural network with dynamic external memory" 
through the addition of dynamic memory allocation and de-allocation for memory reuse. Since then
new types of memory networks have been proposed for few shot learning and meta learning. Specifically, 
"Learning to Remember Rare Events" and "Matching Networks for One Shot Learning" both apply
differentiable memory modules for few shot learning on the Omniglot dataset.
